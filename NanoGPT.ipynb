{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyGVs2hER2gJ",
        "outputId": "a1babd90-a8ec-49f3-8096-c7fcd4465b97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-27 12:16:50--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-12-27 12:16:50 (34.0 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "S2TzolHUaPko"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gU30OG_6SMKY"
      },
      "outputs": [],
      "source": [
        "#read it in to inspect it\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCdQBtNwSw5p"
      },
      "outputs": [],
      "source": [
        "print(\"Length of the dataset in characters: \", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyqKMowYTCV2"
      },
      "outputs": [],
      "source": [
        "#lets look at the first 1000 characters\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6M8wQVckTHcp"
      },
      "outputs": [],
      "source": [
        "#here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# print(''.join(chars))\n",
        "# print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Cwx_FWuLUGSx"
      },
      "outputs": [],
      "source": [
        "#create a mapping from characters to integers\n",
        "stoi = { ch:i for i, ch in enumerate(chars)}\n",
        "itos = { i:ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]   #encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l])   #decoder#: take a a list of integers, output a string\n",
        "\n",
        "# print(encode('hi there'))\n",
        "# print(decode(encode('hi there')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "owcvYk0YVyMZ"
      },
      "outputs": [],
      "source": [
        "#lets now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch  #we use Pytorch: https://pytorch.org\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "# print(data.shape, data.dtype)\n",
        "# print(data[:1000]) # the 1000 characters we looked at earlier will be given to the GPT look like this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BdLtI85oX1xe"
      },
      "outputs": [],
      "source": [
        "#lets now split up the data into train and validation sets\n",
        "n = int(0.9*len(data))  #first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NKznKRoY-Va"
      },
      "outputs": [],
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEyp3TTwanuF"
      },
      "outputs": [],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  # print(f\"when input is {context} the target is: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LSQygM7AbZ-y"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 64 # 32 for small network # how many independent sequences will be processed in paralell?\n",
        "block_size = 256 # for small netwok 8  # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500 # 300\n",
        "learning_rate = 3e-4 # for small network 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embed = 384 # for small network 32\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "def get_batch(split):\n",
        "  # generate a small batch of data of inputs x and target y\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data)-block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "# print('inputs: ')\n",
        "# print(xb.shape)\n",
        "# print(xb)\n",
        "# print('targets: ')\n",
        "# print(yb.shape)\n",
        "# print(yb)\n",
        "\n",
        "# print('----')\n",
        "\n",
        "# for b in range(batch_size): # batch dimension\n",
        "#   for t in range(block_size): # time dimension\n",
        "#     context = xb[b, :t+1]\n",
        "#     target = yb[b, t]\n",
        "#     print(f\"when input is {context.tolist()} the target: {target}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpgACejvf8Tl"
      },
      "outputs": [],
      "source": [
        "print(xb) #our input to the transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XV2T_75FjD3K"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # each token directly reads off the logits for the next token from a lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "    self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "\n",
        "    #idx and targets are both (B,T) tensor of integers\n",
        "    token_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "    # pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "    x = token_emb #+ pos_emb # (B,T,vocab_size)\n",
        "    logits = self.lm_head(x)  # (B,T,vocab_size)\n",
        "\n",
        "    if targets == None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T) #or targets.view(-1)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      # get the predictions\n",
        "      logits, loss = self(idx)\n",
        "      # focus only on the last timestep\n",
        "      logits = logits[:, -1, :] # becomes (B, C)\n",
        "      # apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "      # sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      # append sampled index to the running sequence\n",
        "      idx = torch.cat([idx, idx_next], dim=1) # (B, T+1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "model.to(device)\n",
        "logits, loss = model(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=100)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzuPLb9ClDUR"
      },
      "outputs": [],
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "92A3afdi4bm6"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0XiPcbbzChg"
      },
      "outputs": [],
      "source": [
        "# batch_size = 32\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  # every once in a while evaluate the loss on train and val sets\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  # sample a batch of data\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  # evaluate the loss\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AV9AJ7nSz4_3"
      },
      "outputs": [],
      "source": [
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp7wkCRA9vDb"
      },
      "source": [
        "**The mathematical trick in self-attention**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpm6hRFr01sg"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1337)\n",
        "B, T, C = 4, 8, 2 # batch, time, channels\n",
        "x = torch.randn(B, T, C)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgyXoER0-VLN"
      },
      "outputs": [],
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B, T, C))\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xprev = x[b, :t+1] # (t, C)\n",
        "    xbow[b, t] = torch.mean(xprev, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZVkx6VdCcKH"
      },
      "outputs": [],
      "source": [
        "wei = torch.tril(torch.ones(T,T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (T, T) @ (B, T, C) ---> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3s_apoNBMe-"
      },
      "outputs": [],
      "source": [
        "xbow[0], xbow2[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcNEhmnOD2LO"
      },
      "outputs": [],
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril==0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kjj13NEkB6VK"
      },
      "outputs": [],
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B, T, C = 4, 8, 32  # batch, time, channel\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "# lets see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)  # (B, T, 16)\n",
        "q = query(x)  # (B, T, 16)\n",
        "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ----> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "# wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril==0, float('-inf')) # this line is specific to decoder blocks and is deleted for encoder blocks\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "# out = wei @ x\n",
        "\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3p1uiFnBFy_2"
      },
      "outputs": [],
      "source": [
        "wei[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQ5CZxnu_wAt"
      },
      "outputs": [],
      "source": [
        "x[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZotefbS_Kyc"
      },
      "outputs": [],
      "source": [
        "xbow[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQKwQs3A_jrC"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3,3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10, (3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('b=')\n",
        "print(b)\n",
        "print('c=')\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "f-vKzXSVBWDV"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "  \"\"\" one head of self-attention \"\"\"\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    k = self.key(x)  # (B, T, C)\n",
        "    q = self.query(x)  # (B, T, C)\n",
        "    # compute attention scores (\"affinities\")\n",
        "    wei = q @ k.transpose(-2, -1) * C ** -0.5 # (B, T, C) @ (B, C, T) ----> (B, T, T)\n",
        "    wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf')) # (B, T, T)\n",
        "    wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "    wei = self.dropout(wei)\n",
        "    # perform the weighted aggregation of the values\n",
        "    v = self.value(x) # (B, T, C)\n",
        "    out = wei @ v # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nAvFMHdaXNbs"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embed, n_embed)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dXLU6B40mdj6"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "  \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "  def __init__(self, n_embed):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embed, 4 * n_embed),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embed, n_embed),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fa8AXxnpolu8"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "  \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "  def __init__(self, n_embed, n_head):\n",
        "    # n_embed: embedding dimension, n_head: the number of heads we'd like\n",
        "    super().__init__()\n",
        "    head_size = n_embed // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffw = FeedForward(n_embed)\n",
        "    self.ln1 = nn.LayerNorm(n_embed)\n",
        "    self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffw(self.ln2(x))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "eiEuzMTgRbiL"
      },
      "outputs": [],
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # each token directly reads off the logits for the next token from a lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "    # self.sa_head = Head(n_embed)\n",
        "    # self.sa_heads = MultiHeadAttention(4, n_embed//4) # i.e. 4 heads of 8-dimensional self-attention\n",
        "    # self.ffw = FeedForward(n_embed)\n",
        "    # self.blocks = nn.Sequential(\n",
        "    #     Block(n_embed, n_head=4),\n",
        "    #     Block(n_embed, n_head=4),\n",
        "    #     Block(n_embed, n_head=4),\n",
        "    #     nn.LayerNorm(n_embed)\n",
        "    # )\n",
        "    self.blocks = nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embed) # final layer norm\n",
        "    self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "\n",
        "    #idx and targets are both (B,T) tensor of integers\n",
        "    token_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "    x = token_emb + pos_emb # (B,T,vocab_size)\n",
        "    # x = self.sa_head(x)\n",
        "    # x = self.sa_heads(x) # apply one head of self-attention. (B, T, C)\n",
        "    # x = self.ffw(x) # (B, T, C)\n",
        "    x = self.blocks(x)  # (B, T, C)\n",
        "    x = self.ln_f(x)  # (B, T, C)\n",
        "    logits = self.lm_head(x)  # (B,T,vocab_size)\n",
        "\n",
        "    if targets == None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T) #or targets.view(-1)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      # crop the idx to the last block_size tokens\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      # get the predictions\n",
        "      logits, loss = self(idx_cond)\n",
        "      # focus only on the last timestep\n",
        "      logits = logits[:, -1, :] # becomes (B, C)\n",
        "      # apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "      # sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      # append sampled index to the running sequence\n",
        "      idx = torch.cat([idx, idx_next], dim=-1) # (B, T+1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6wEiFPwao8E",
        "outputId": "5d617dc9-43ef-412e-f669-d08aca8f5b97"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BigramLanguageModel(\n",
              "  (token_embedding_table): Embedding(65, 384)\n",
              "  (position_embedding_table): Embedding(256, 384)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffw): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffw): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (2): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffw): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (3): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffw): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (4): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffw): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (5): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffw): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): Linear(in_features=384, out_features=65, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "model = BigramLanguageModel()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "vaY8qIwTi1we"
      },
      "outputs": [],
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZFyKT1wUQsi",
        "outputId": "c390ce28-d67a-4755-c8da-cbbdb1fd702a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.3360, val loss 4.3324\n",
            "step 500: train loss 2.0122, val loss 2.0957\n",
            "step 1000: train loss 1.6069, val loss 1.7830\n",
            "step 1500: train loss 1.4399, val loss 1.6363\n",
            "step 2000: train loss 1.3467, val loss 1.5719\n",
            "step 2500: train loss 1.2840, val loss 1.5366\n",
            "step 3000: train loss 1.2302, val loss 1.5084\n",
            "step 3500: train loss 1.1866, val loss 1.4909\n",
            "step 4000: train loss 1.1489, val loss 1.4779\n",
            "step 4500: train loss 1.1115, val loss 1.4790\n",
            "1.1526715755462646\n"
          ]
        }
      ],
      "source": [
        "for iter in range(max_iters):\n",
        "\n",
        "  # every once in a while evaluate the loss on train and val sets\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  # sample a batch of data\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  # evaluate the loss\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uQjnhhmiVrQ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc60b154-fca3-4881-e88e-0bd1290fdff8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "By your father's party; steel win thou,\n",
            "merch; lamentables, allow'd and flesh arm more.\n",
            "\n",
            "PETER:\n",
            "Peace, and to Haple, no: you have stand about me\n",
            "As mill you be his justice of him.\n",
            "\n",
            "PETH:\n",
            "Make home good and you have no holy lace;\n",
            "Whose truth vengeans of you, lords as his\n",
            "favour tears for my guilty affects, while they\n",
            "do near us.\n",
            "\n",
            "Marken:\n",
            "His gentleman truths sun, all men leads pon their.\n",
            "\n",
            "Second Barina:\n",
            "The wert'st they shall be strew his man? I'll not see\n",
            "this of desire in farth: the maids he na\n"
          ]
        }
      ],
      "source": [
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "OXVdVZ38ZuTO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d363f01-5a78-41a0-f777-46172466a88d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ING see how of Elysigh king hath grows my heart;\n",
            "For like him to heir shades his curse and grows ;\n",
            "And darince give him and hurl your fact.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "You divy, my lord, I'ld speak no gentle fault.\n",
            "Cousin wish.\n",
            "\n",
            "Second Petruch:\n",
            "You shall go begg'd to thim; farewell.\n",
            "\n",
            "HASTINGS:\n",
            "I hope my lords, is but fossock, minking and you?\n",
            "\n",
            "CLARENCE:\n",
            "Second murders, I have you want.\n",
            "\n",
            "LARTIUS:\n",
            "O good in goodmandanstay;\n",
            "You pray'd by Coriolanus sweet here, and all,\n",
            "Never deceit to good upon your purposture,\n",
            "And then longet the perpour of his name\n",
            "Nor than I have a little father, and in lamons,\n",
            "Then show'd musign on my breast dancing and\n",
            "Stand appointed word their drunks,\n",
            "And I would chare the league\n",
            "To break o'er the noseperishmen hercy,\n",
            "She'll appear it in love.\n",
            "\n",
            "Second Citizen:\n",
            "Farewell:\n",
            "We barbend you; there clears you?\n",
            "\n",
            "Shepherd:\n",
            "Sir, no sir, in a gentleman command,\n",
            "That no common lie tormurelled by\n",
            "By weeping by cheeks. Fhow my crown heart.\n",
            "\n",
            "WARWICK:\n",
            "Somewife! are the matter all the nobles purgage;\n",
            "Where flours make kissing me by my deareful lawful\n",
            "From off these starpetulate; and what thou had praised\n",
            "Some ralmong me with oaken coucked from life.\n",
            "\n",
            "WARWICK:\n",
            "O, you sleep here certain, I am, not proud,\n",
            "Will we but possession and suntler to your arm.\n",
            "\n",
            "NORTHBERLAND:\n",
            "They shall well, yet here with you shall: all in\n",
            "their country beares out.\n",
            "Come, you have for womane; you have a servant.\n",
            "\n",
            "BAGOT:\n",
            "Go, good lord Pity, coz, for if you be furthen's\n",
            "Endured with where\n",
            "And, by your channured woman, my noble duty unnevorce\n",
            "But  would a cause serve woman.\n",
            "So, good lord; we must cannot suck effer:\n",
            "The time of the sun Ollicar devile\n",
            "Do spent the seate that I owe well acquaint.\n",
            "\n",
            "ROMEO:\n",
            "I thank the thing who worldlow'st plumely.\n",
            "\n",
            "CATESBY:\n",
            "Peace, my gracious make be riddoned, and jocund;\n",
            "'tis we cmongmand an honourable of command.\n",
            "Salisbury, and take these fresh in leances\n",
            "Within eyes, cheer answered before his son:\n",
            "He shall give his lieour rich and ghostly gird\n",
            "A petty heart; end he slander again to\n",
            "Attend the riverers of me here your gartes.\n",
            "\n",
            "GLOUCESTER:\n",
            "The great of Bolingbroke?\n",
            "\n",
            "QUEEN MARGARET:\n",
            "That's young feel, dost he promit them deny.\n",
            "Addle more deck that I might hear mean,\n",
            "But quess'd without mayst attacher.\n",
            "Or Bishop art Boy, good Clarence the palace:\n",
            "Hence I live the battle respect. We mis banish:\n",
            "Mine own squickly and Jointes, go at\n",
            "Edward so contrue the day of the bearer;\n",
            "That lies for a-day's dear; never expulled my\n",
            "inquiscience she be my spirit.\n",
            "\n",
            "CAMILLO:\n",
            "For, that.\n",
            "He consul the widows flesh is sailthood, about it,\n",
            "Whenty and the hand previse and he hath tell he\n",
            "Old the Duke of Gaunt, and to his;  for when,\n",
            "She fight the drunk, as he did on he with\n",
            "So step and rib present with the Funtionher;\n",
            "And by other depty not in men's death.\n",
            "\n",
            "RICHARD:\n",
            "Perhappetive we still thus mock--\n",
            "Somet love, for thy oath tongue the ear\n",
            "You tarnethe and dangered let me prepare.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "O, mink God what doth stand, in thou man!\n",
            "So, thou, death with me, and let thinks planter;\n",
            "That's signify time thou hast with frown to pine in\n",
            "sole in ventness. Gallous, tempery thy name,\n",
            "To endure thee we prayer. Come, trust, his ones,\n",
            "Who break from me sharp thy rovenges' sacreet\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "O young daught is sportess.\n",
            "\n",
            "KING RICHARD III:\n",
            "Were murderer, be pilg Richard as the woman,\n",
            "Was I live thee against in earseman.\n",
            "I scarve thee beggars of transger to thy combass?\n",
            "Now, for my colours, those I seep,\n",
            "To durst it will break of thee private\n",
            "My battle's dear lewdnim; to what my love\n",
            "I must redislike my wipe.\n",
            "\n",
            "BUCKINGHAM:\n",
            "Lord my counself, for we never will betrod\n",
            "Have proclaiment'd these such as even here\n",
            "Will underserver lie to me; your could sigh you\n",
            "Selves your lusty rosofs. Draw your wits,\n",
            "Like i' wit a past: the wervive this takentry.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "All, but I'ld not please you; and you smarch\n",
            "'Devoid not the Paulina course.' What must!\n",
            "We have like our bury, his head's honesty cause,\n",
            "Who are a traitor coron! Montague than I would be\n",
            "To watch more than a Christian drums' head.'\n",
            "A piece, my mastering lord,\n",
            "He both that holbour in that cause with ashe\n",
            "Unless the empty that you him.\n",
            "\n",
            "SICINIUS:\n",
            "We were beseech your thing.\n",
            "\n",
            "CORIOLANUS:\n",
            "Whither on my father?\n",
            "\n",
            "CORIOLANUS:\n",
            "Which we no less;\n",
            "Twell, unseen says be long: smale the fickless\n",
            "Speak of his known griples profess'd it from\n",
            "The gentle.\n",
            "\n",
            "COMINIUS:\n",
            "He's young away to part as unfoldity?\n",
            "\n",
            "BRUTUS:\n",
            "Ay, sirrah, 'no, task.\n",
            "\n",
            "CORIOMEO:\n",
            "Promised, sir;\n",
            "The laguage forne of Baynishmen's head,\n",
            "He will be pass'd for far-in the time.\n",
            "\n",
            "ROMEO:\n",
            "My lord,\n",
            "Sir, for most, I hoping my husband!\n",
            "\n",
            "CLEOMENENTIO:\n",
            "Whither, thank you'll revost\n",
            "We are going hereafts much.\n",
            "\n",
            "PAULINA:\n",
            "This is that, which I can see,\n",
            "Contrivinectizens me, scendance; who,\n",
            "To suppose for the king back and mock the well:\n",
            "Confess me, green lord with like the sanctuary,\n",
            "That ender one day of brear!\n",
            "Her else some particular\n",
            "Hath a state of battaphons' bloody and the\n",
            "fibured, being the rootsest with but floodse, who,\n",
            "Were likely a bearing of our honour woes reported\n",
            "To respect it: if blessed is his hisgood\n",
            "With was neathely of our watch consul,\n",
            "Which of God's royal heard in Buckingham,\n",
            "And his course to Henry shriving him suck;\n",
            "But lovely the earth alth. How deplay\n",
            "Be straight of his youth day flower, who' slaves\n",
            "To the hare: we princeless you as little Roman.\n",
            "\n",
            "LEONTES:\n",
            "How goes his love?\n",
            "\n",
            "HERMIONE:\n",
            "I moven before you?\n",
            "\n",
            "MILLIUS:\n",
            "And let him because me.\n",
            "\n",
            "MARTIUS:\n",
            "I go, poople, in but one wonders are they in you.\n",
            "\n",
            "LADY CAPULET:\n",
            "Angelo stay good much, in good caut it in;\n",
            "And let be with wife by patience fears. Eithere's thought is so\n",
            "A one that in your daughter: report stars I\n",
            "To reverend the yoning moral;\n",
            "But fear you know I say now, sisteeds about\n",
            "Your happing in all. Call burgus hither?\n",
            "You must not up your lordship fear.\n",
            "\n",
            "HASTINGS:\n",
            "It will not fly your guests for age.\n",
            "But now, she lays go come 'to the bust.\n",
            "\n",
            "KING RICHARD III:\n",
            "Ay, good lads, and be so.\n",
            "\n",
            "KING HENRY VI:\n",
            "My heart is mind thy slow mercy:\n",
            "He that smooth from my soul force the grant\n",
            "That had enough a guard doing.\n",
            "\n",
            "KING HENRY VI:\n",
            "Ay, my lord of Warwick, and you may.\n",
            "She lieked Angelo live, but on bite speedy with\n",
            "By themselves.\n",
            "\n",
            "PERDITA:\n",
            "I hate no gravel teach of this grief:\n",
            "Come on my knock, sir, some quietness: call me woman.\n",
            "\n",
            "LORD STANLEY:\n",
            "The wrong-day I hate pointed clogs; for an broing forth\n",
            "great till tie a part; was, the thing will-spoker\n",
            "person and gave the deserver's wide fray.\n",
            "Therefore, no, be thankethou thus'st rage: yet not fone son,\n",
            "'tis not your honour.' I have spent; my baby my bradare touchless\n",
            "against toward; and a glad as would we lave\n",
            "an three too?\n",
            "\n",
            "AUTOLYCUS:\n",
            "Have me the from leave I thee now doke.\n",
            "\n",
            "She:\n",
            "Hold Angelo good moody!\n",
            "\n",
            "Clown:\n",
            "No; ply I think you to honour burn enough.\n",
            "\n",
            "AUFIDIUS:\n",
            "All follows to the Claudio's sight.\n",
            "\n",
            "CORIOLANUS:\n",
            "I could bellame, for of I crock to breathe,\n",
            "That onck you of the that is he intered:\n",
            "The mother of the horonous all-saints\n",
            "Of blockary buzzzaled astainess.\n",
            "\n",
            "Clown:\n",
            "You do you no morrow.\n",
            "\n",
            "VOLUMNIA:\n",
            "Come hither, let's him. Come, sir.\n",
            "Come, come? And sir, hither.\n",
            "\n",
            "VIRGILIA:\n",
            "He had a queen in a paison grief.\n",
            "\n",
            "PAULINA:\n",
            "I would show his a progoming\n",
            "Was lightning has better.\n",
            "\n",
            "CORIOLANUS:\n",
            "O, at Phaethon more,\n",
            "That will upass his body; and in his royal grand\n",
            "On his virtuous danquon; a vow, ay,\n",
            "More denying so mischances his.\n",
            "\n",
            "ANGELO:\n",
            "Refuse:\n",
            "Good fellow; be call'teous sweeter.\n",
            "Cheerh you find your majesty!\n",
            "To whom come wooers' foot his purposession! But, folguids,\n",
            "And sold tim rule to grave grief, and tell you go\n",
            "To Tue holds, you must have no call it.\n",
            "\n",
            "GRUMIO:\n",
            "Because you all frownice you mewish: we shall speak.\n",
            "\n",
            "LUCENTIO:\n",
            "The matters of my successip of death.\n",
            "\n",
            "ANTIGONUS:\n",
            "Hark you not pass, you'r estage; nay?\n",
            "\n",
            "LUCIO:\n",
            "I do not possible in sits as and succh as fear\n",
            "they shall put it geences: think it you her\n",
            "one estile part. Claster, the gods unins\n",
            "appopule it is in; his caught have found bleenr, whe do\n",
            "accuse to her. Camillous; for which he did her: will\n",
            "rettire you our country sorrown told her\n",
            "carnot is circuple. By her be! now God, my lord,\n",
            "we would do beseech you thus?\n",
            "\n",
            "POLINGES:\n",
            "O, the must be;\n",
            "He loves you see then fair frails?\n",
            "\n",
            "HENRMIONE:\n",
            "I' shall beal'd at gold or to have\n",
            "Our pensecians: 'follow's honours' king. I am\n",
            "A mourning bewrit upon the anqueens queen'd,\n",
            "Backing e'en so: now is the strength receive'd\n",
            "Ties with our brack sands. Thou dareth slingth in leave.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "The king is't indeed in miseable,\n",
            "Can the Each of Warwick days agains,\n",
            "And see Richarge did bear:\n",
            "I'er her hum garden'd with no leave.\n",
            "\n",
            "HASTINGS:\n",
            "Yiet follow to Bentonius, I'll do we come.\n",
            "\n",
            "MARCIUS:\n",
            "Hark you? any stay up the gates?\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Owe!\n",
            "\n",
            "DERBY:\n",
            "Young words, gentle mind, say 'tis more traitor.\n",
            "\n",
            "KING RICHARD II:\n",
            "We do hope it, poorf.\n",
            "\n",
            "NORFOLBURvALEY:\n",
            "Sir, constant; come, bad him strong that the Duke of:\n",
            "Now blessister speed yet wakes'd it, drum not.\n",
            "\n",
            "Nurse:\n",
            "Welcome and Lord Hastify Barner, watchman?\n",
            "\n",
            "KING RICHARD IOLAND:\n",
            "Ah, Juliet!\n",
            "Did! for judgerous Aufidius gate!\n",
            "Who hath curn here her long fitters in pile,\n",
            "A teacherous warlould him of made looks,\n",
            "And when 'twiche's modeeble are preyouse.\n",
            "\n",
            "HASTINGS:\n",
            "Command me unto: she was not king her?\n",
            "\n",
            "PERDITA:\n",
            "Go tell he's sengled for wonty herer roya.\n",
            "\n",
            "PETRUCHIO:\n",
            "No, no more; no; not, if they were come to bite them,\n",
            "Where should command not command the Volscians;\n",
            "One, they are in reasons as it.\n",
            "\n",
            "CORIOLANUS:\n",
            "Pray you, come, my good my name: I pray you,\n",
            "A pleasure of God mantual by gold;\n",
            "The king not of the faults common out them shipple.\n",
            "\n",
            "LUCIO:\n",
            "I confess it. Do thou help' love thy turn lords,\n",
            "'Twice in any perfixed horse pound and witness,\n",
            "Since our Heartingbrowman's never brother,\n",
            "So sweet upon her remembers' heart's speech brother:\n",
            "Within thee, even so, or shall proclaim us\n",
            "Without unstruminedly, and fasts you go.\n",
            "I leave the population craves into their words\n",
            "have stock good less, and you shall in flield;\n",
            "But, he is not so, yours; neve--mought you so have\n",
            "Surple, he shall be all fromonged to c\n"
          ]
        }
      ],
      "source": [
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=10000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i_10VOAnNl1g"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}